\documentclass[twoside,openright,titlepage,numbers=noenddot,headinclude,%
               footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,%
               paper=a4,fontsize=11pt,ngerman,american]{scrreprt}

% Custom config ===============================================================

% Classic thesis
\usepackage{amssymb}
\input{classicthesis-config}

% Theorems and definitions
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\newtheorem{algorithm}{Algorithm}
\usepackage{algpseudocode}

% Counters
\renewcommand{\labelenumi}{{\color{halfgray}(\alph{enumi})}}
\renewcommand{\labelenumii}{\color{halfgray}{\roman{enumii}.}}
\renewcommand{\labelitemi}{{\color{halfgray}-}}%\raisebox{0.3ex}{\tiny$\blacksquare$}}}

\numberwithin{theorem}{chapter}
\numberwithin{definition}{chapter}
\numberwithin{algorithm}{chapter}
\numberwithin{figure}{chapter}
\numberwithin{table}{chapter}

% Maths
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\numberwithin{equation}{chapter}
\allowdisplaybreaks

% Shaded boxes
\usepackage{framed}
\newenvironment{remark}[1]{%
  \definecolor{shadecolor}{gray}{0.9}%
  \begin{shaded}{\color{Maroon}\noindent\textsc{#1}}\\%
}{%
  \end{shaded}%
}

% Code snippets
%\usepackage{color}

\usepackage{color}
\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\definecolor{bgcolor}{rgb}{1.0,1.0,1.0}
%\newminted{python}{bgcolor=bgcolor}

% Todo
\newcommand{\todo}[1]{\textcolor{red}{[TODO] #1}}

% PS pictures
%\usepackage{pstricks,auto-pst-pdf}

% For the images and graphics
\usepackage{subfig} % For subfigures in floats
\usepackage[section]{placeins}
\makeatletter
 \@ifpackageloaded{tex4ht}{%
\usepackage[dvips]{color,graphicx}
    \usepackage[tex4ht]{hyperref}
    }{%
      \usepackage[pdftex]{graphicx}
      \usepackage{hyperref}
          }
\makeatother
\graphicspath{ {figures/} } %Path to images


% Landscape tables
\usepackage{rotating}

% Checkmarks
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Wide tables
\usepackage{ltablex}


% -----------------------------------------------------------------------------

\begin{document}
\frenchspacing
\raggedbottom
\selectlanguage{american}
\pagenumbering{roman}
\pagestyle{plain}


% Front pages =================================================================
\include{titlepage}

% Content =====================================================================
\pagenumbering{arabic}

\cleardoublepage

%----------------------------------------------------------------------------------------
%  CHAPTER CONTENTS
%----------------------------------------------------------------------------------------

\chapter*{Introduction}

\section*{Project Overview}

This project applies unsupervised learning techniques on product spending data collected for customers of a wholesale distributor in Lisbon, Portugal to identify customer segments hidden in the data. 

\section*{Problem Statement}

A wholesale distributor recently tested a change to their delivery method for some customers, by moving from a morning delivery service five days a week to a cheaper evening delivery service three days a week. Initial testing did not discover any significant unsatisfactory results, so they implemented the cheaper option for all customers. Almost immediately, the distributor began getting complaints about the delivery service change and customers were canceling deliveries--losing the distributor more money than what was being saved.

The goal of the wholesale distributor is to find what types of customers they have to help them make better, more informed business decisions in the future.

The dataset contains 440 data points representing clients of a wholesale distributor. It includes the annual spending in monetary units on the following product categories:

\begin{itemize}% 
\item Fresh
\item Milk
\item Grocery
\item Frozen
\item Detergents\_Paper
\item Delicatessen
\end{itemize}



To find out what types of customers the wholesale distributors have we suggest the following strategy:
\begin{enumerate}% 
\item Preprocess the data by applying appropriate techniques like feature scaling and outlier detection.
\item Explore the data to determine relevant features.
\item Perform a principal component analysis on the data to understand which features seem to trend together.
\item Implement clustering to find hidden patterns in a dataset.

\end{enumerate}



\section*{Metrics}
For this problem, we don't know how many ``segments'' of customers there are. In fact, this is what we are trying to learn. As such, we can use the \emph{silhouette score} of each data point to determine how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). The \emph{mean silhouette coefficient score} can the serve as a metric that can guides us in learning the optimal number of segments in the data.



%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Analysis}

\section*{Data Exploration}

The dataset used in this project was created by UC Irvine's' Center for Machine Learning and Intelligent Systems. The data is stored in a CSV file where each row corresponds to an order.

\section*{Sample Data Points}
To gain a better understanding of the customers and how their data will transform through the analysis, we randomly selected three data points that varied significantly from each other. 

\begin{table}[!htbp]
  \begin{center}
  \caption{Samples' category spending.}
    \begin{tabular}{ |l|l|l|l|l|l|l| } 
    \hline
    Sample & Fresh & Milk & Grocery & Frozen & Detergents\_Paper & Delicatessen\\[1ex]

    \hline
    
      0 &12669 &9656  &7561  &214   &2674  &1338 \\
      1 &3067  &13240 &23127 &3941  &9959  &731  \\
      2 &32717 &16784 &13626 &60869 &1272  &5609 \\
    \hline
    \end{tabular}
  \end{center}
\end{table}

\begin{figure}[!hbtp]
\centering
    \includegraphics[width=0.9\textwidth]{figures/percentileRanksSampleSpending}
    
    
    \caption{\textbf{Percentile ranks of samples' category spending. }}\label{percentileRanksSampleSpending}
\end{figure}




\section*{Sample Description}
\label{sampleDescription}

As a means of understanding and validating the usefulness of the learned customer segments, we predicted the kind of establishment (customer) each of the three samples represent.

\begin{itemize}
\item \textbf{Sample 0} \\
From the heat-map in figure \ref{percentileRanksSampleSpending} we can see that this sample represents an establishment that spends very little on texttt{Frozen} category as compared to other product categories. Furthermore, this kind of establishment also spends significantly on texttt{Milk}, at eighty-three percentile to be exact. Our intuition leads us to think that this \emph{might} be a \textbf{market}. 

\item \textbf{Sample 1} \\ 
This kind of establishment spends less on texttt{Delicatessen} at forty-first percentile and even significantly less on the texttt{Fresh} category. On the other categories, their spending is at well over the seventieth percentile range. Our intuition leads us to believe this might be a place that serves meals and offers lodgings like a \textbf{hotel} or a \textbf{university} with a residence halls and adjoining cafeteria. 

\item \textbf{Sample 2} \\
This establishment spends significantly on all categories with the exception of the \texttt{Detergents\_Paper} category where their spending is limited to the fifty-eight percentile. Our guess is that this sample represents \textbf{wholesale retailer} like a ``Costco" or ``Sam's club".

\end{itemize}



\section*{Feature Relevance}
We can determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.

We predicted the impact of the feature \texttt{Grocery} on customers' spending habits. The prediction score was 0.682 out of 1. This score which tends towards being high indicates that it this feature is not necessary for identifying customers' spending habits. The reason is as follows: 
\begin {itemize}
\item when customers spend on \texttt{Grocery} they also spend on other features, which means we can't decipher spending habits. 
\end{itemize}
What would be better is to regress on a feature that has a \textbf{low} $R^2$ score indicating low correlation.


To get a better understanding of the dataset, we can construct a scatter matrix of each of the six product features present in the data. From figure \ref{scatterplot} we can see that the data for these features is not normally distributed. Its skewed to the right and has a long tail.
\begin{figure}[!hbtp]
\centering
   
    \includegraphics[width=0.9\textwidth]{figures/scatterplot}
    
    
    \caption{\textbf{A scatter matrix for each pair of features in the data.}}\label{scatterplot}
\end{figure}

The pairs \texttt{Grocery} and \texttt{Detergents\_Paper} have a very strong correlation. From the scatterplot, one can see that the data for these two features looks like it could fit a nice diagonal. To formalize my intuition, I performed a pearson's correlation on these two features and got a 0.92 score, noting that a score of 1 is the highest a correlation can be.

Another pair was \texttt{Grocery} and \texttt{Milk}. From the scatterplot, you can kind of see a trending along the diagonal. It had a correlation score of 0.73.

Another pair was \texttt{Milk} and \texttt{Detergents\_Paper}, the pair had a correlation score of 0.66. This confirms my suspicions that \texttt{Grocery} is **not** a strong predictor of overall client spending habits.




\section*{Outlier detection}
The algorithm that we will be using to cluster the data points, \texttt{KMeans} is \emph{very sensitive} to outliers. Since this algorithm uses the distance from the centroids of the clusters, i.e. the means to calculate which data points belong to what cluster, leaving values that are very far off from the rest will influence the structure, leading to undesirable clusters.

As part of the data exploration process, we were careful to analyze the data for potential outliers. We used Tukey's Method for identfying outliers to clean the data. We removed 42 data points from our dataset which corresponds to 10\% of the dataset. In this case we still have around 390 uniques data points for a problem with 6 variables. 


%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------
\chapter*{Methodology}

\section*{Feature Transformation}
We scaled the data to a more normal distribution and  removed outliers. We then applied PCA to the data to discover which dimensions about the data best maximize the variance of features involved. 

\begin{figure}[!hbtp]
\centering
    
    \includegraphics[width=0.9\textwidth]{figures/pca}
    \label{pca}
    
    \caption{\textbf{PCA on the log transformed data. }}
\end{figure}

\begin{figure}[!hbtp]
\centering
    
    \includegraphics[width=0.9\textwidth]{figures/pcaCorrelation}
    \label{pcaCorrelation}
    
    \caption{\textbf{Correlations on the PCA log transformed data. }}
\end{figure}


Unsurprisingly, dimensions 1 and 2 account for almost 72\% of the variance in the data. One of the interesting aspects of PCA is that the most interesting dynamics occur only in the first $k$ dimension, and then they fall off. In this case, I would say the first \emph{3} components highlight some clear customer segments.    

The first four principal components account for 93\% of the variance in the dataset. For this analysis, we have decided that a correlation value above 0.5+- is deemed significant. 

\begin{itemize}

\item First Principal Component Analysis - Dimension 1 \\
The first principal component is made up of large positive weights in \texttt{Detergents\_Paper}, and lesser but still sizable positive weights on \texttt{Grocery} and \texttt{Milk}. It also correlates with a decrease in \texttt{Fresh} and \texttt{Frozen}. This might represent spending in household staples products that are purchased together. 

It is important to note that we have a lot of variance in customer spending in the correlated features of \texttt{Detergents\_Paper}, \texttt{Grocery} and \texttt{Milk} -- some customers buy more while other customers buy less in these three categories.

\item Second Principal Component Analysis - Dimension 2\\
The second principal component is made up of large positive weights in \texttt{Fresh} and \texttt{Frozen}, a lesser but still sizable weight on \texttt{Delicatessen}, and smaller weights on \texttt{Milk} and \texttt{Grocery}. It also correlates with a slight decrease in \texttt{Detergent\_Paper}. This might represent spending in food items that are purchased together.

\item Third Principal Component Analysis - Dimension 3\\
The third principal component is made up of a large positive weight in \texttt{Fresh}, a small positive weight in \texttt{Detergents\_Paper}, and a significantly lesser positive weight in \texttt{Grocery}. Also, the third principal component has a large negative weight in \texttt{Delicatessen} and a lesser negative weights in \texttt{Milk} and \texttt{Frozen} categories. This might represent spending more in the \texttt{Fresh} category while spending less in \texttt{Delicatessen} category.

We have some variance in customer spending in the correlated features of \texttt{Fresh}, \texttt{Detergents\_Paper} and \texttt{Grocery} -- some customers buy more while other customers buy less in these 3 categories.


\item Fourth Principal Component Analysis - Dimension 4\\
The fourth principal component is made up of a large positive weight in \texttt{Frozen} and a lesser but somewhat sizable positive weight in \texttt{Detergents\_Paper}. Also, this principal component has large negative weight in \texttt{Delicatessen}, a smaller negative weight in \texttt{Fresh}, \texttt{Milk}, and a significantly smaller negative weight in \texttt{Grocery}. This might represent spending more on non-perishable products while simultaneously spending less on perishable products.
\end{itemize}

The first dimension seems to represent customers who spend \emph{significantly} on \texttt{Detergents\_Paper}, spend okay on \texttt{Milk} and \texttt{Grocery}, and none to less on \texttt{Fresh} produce and \texttt{Frozen} products. Another way to understand these first four dimensions is that they each represent a \emph{customer segment}

\section*{Implementation}

Because the first two dimension represent a vast amount of explained variance in the data, we reduced the dataset down to those dimensions. We then performed clustering on the reduced dataset. 

For clustering, we selected the \textbf{KMeans} algorithm for the following reasons:
\begin{itemize} 
\item We want to have a clear delineation of who the various customer segments are. KMeans gives hard partitions and will allow for this.  
\item KMeans scales and is a simple and efficient algorithm unlike the Gaussian Mixture Model which does not scale. If we ever grow the dataset, this difference will save a lot of time in the future.  
\item Easy to understand and compute
\item It's efficient. The time requirement is $O(I*K*m*n)$ as long as the number of clusters is significantly less than m.
  \begin{itemize} 
    \item where $I$ is the number of iterations required for convergence
    \item where $K$ is the number of clusters
    \item where $m$ the number of points
    \item where $n$ is the number of attributes
    \end{itemize}  
\end{itemize} 

Another algorithm we considered was the Gaussian Mixture Model. Which allows for soft-classification, i.e., data point can belong to more than one cluster with probabilities.
   

We calculated the mean silhouette coefficient for the data points and determined that the optimal number of clusters to use in describing the data was two. 

\begin{figure}[!hbtp]
\centering
    
    \includegraphics[width=0.9\textwidth]{figures/meanSilhouetteScore}
    
    \caption{\textbf{The mean silhouette coefficient score for several hypothetical clusters.}}\label{meanSilhouetteScore}
\end{figure}


\begin{figure}[!hbtp]
\centering
    
    \includegraphics[width=0.9\textwidth]{figures/kMeansModelResults}
    
    \caption{\textbf{Cluster learning on PCA reduced data.} \textit{Centroids are marked by the numbers, while the transformed sample data are marked by the Xs}}\label{kMeansModelResults}
\end{figure}



%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Results}

From the two clusters learned from the implementation of the KMeans algorithm, with respect to the problem of creating customer segments, a cluster's center point corresponds to the average customer of that segment. Our goal then becomes describing the kinds of \emph{establishments} that these cluster centers might be describing.

Focusing on \textbf{Segment 0}, that cluster center seems to describe establishment that spend significantly on fresh and frozen products. From this, we are inclined to believe that establishment that this segment implies might be best described as belonging to the set that includes, but not limited to \textbf{(supermarkets, caterers, restaurants)}.


Also, \textbf{Segment 1}'s spending on milk, grocery, frozen and detergents categories is around or over the 75\% range for this dataset. Based on intuition, we are led to think that this customer segments might best be described as belonging to the set that includes, but not limited to  \textbf{(child care centers, public school systems, hotels, hospitals, universities, military bases)}. 


\begin{table}[!htbp]
  \begin{center}
  \caption{Segments \emph{learned} from clustering.}
    \begin{tabular}{ |l|l|l|l|l|l|l| } 
    \hline
    Sample & Fresh & Milk & Grocery & Frozen & Detergents\_Paper & Delicatessen\\[1ex]

    \hline
    
      Segment 0 &9451  &1938  &2449  &2200  &307 &771 \\
      Segment 1 &5424  &7780  &11532 &1123  &4444  &1136  \\
      
    \hline
    \end{tabular}
    \label{tableSegments}
  \end{center}
\end{table}

\section*{Model Evaluation and Validation}

\begin{figure}[!hbtp]
\centering
    
    \includegraphics[width=0.6\textwidth]{figures/samplePointSegment}
    
    \caption{\textbf{Predicted segments for sample data points.}}\label{samplePointSegment}
\end{figure}

For each sample point, we predicted which customer segment best represents it. From our prediction, all three sample points were put in segment 1 as can be seen from figure \ref{samplePointSegment} 

Based on our intuition, \textbf{Sample 0} doesn't line up. We thought that sample was a small market, and \emph{segment 1} more like a place to caters to feeding and caring for children.

For \textbf{Sample 1} our reasoning holds. If we generalize \emph{child-care center} to institutions that feed people and do a lot of laundry like beddings and table cloths, then it makes sense that \textbf{Sample 1} is assigned to cluster 1.

The predicted cluster for \textbf{Sample 2} doesn't line up. We thought of cluster 1 as a child-care center or restaurant, and \textbf{Sample 2} as a wholesale supermarket like Costo. From the clusters in figure \ref{kMeansModelResults}, we can see that this point is amongst the farthest away from the center, which corresponds to a numerically larger data point, so perhaps it still falls in line with our intuition.


\begin{figure}[!hbtp]
\centering
    
    \includegraphics[width=0.9\textwidth]{figures/regions}
    
    \caption{\textbf{PCA reduced data labeled by \texttt{Channel}.} \textit The transformed sample data is circled. }\label{regions}
\end{figure}


To determine how well our clustering algorithm and number of clusters chosen did, we compared it to the underlying distribution of Hotel/Restaurant/Cafe customers and Retailer customers. The KMeans algorithm with 2 clusters does rather well. It correctly identified the Retailers cluster, and the Hotels/Restaurants/Cafes as is evident from comparing figures \ref{kMeansModelResults} and \ref{regions}.

The Retailer cluster matches nicely to what we referred to as a supermarket. The Hotels/Restaurants/Cafes also matches nicely as we previously said that we thought the place might be a child care center or a large restaurant chain that serves breakfast.
%----------------------------------------------------------------------------------------
%  CHAPTER 



\end{document}  