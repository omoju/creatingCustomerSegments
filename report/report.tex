\documentclass[twoside,openright,titlepage,numbers=noenddot,headinclude,%
               footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,%
               paper=a4,fontsize=11pt,ngerman,american]{scrreprt}

% Custom config ===============================================================

% Classic thesis
\usepackage{amssymb}
\input{classicthesis-config}

% Theorems and definitions
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\newtheorem{algorithm}{Algorithm}
\usepackage{algpseudocode}

% Counters
\renewcommand{\labelenumi}{{\color{halfgray}(\alph{enumi})}}
\renewcommand{\labelenumii}{\color{halfgray}{\roman{enumii}.}}
\renewcommand{\labelitemi}{{\color{halfgray}-}}%\raisebox{0.3ex}{\tiny$\blacksquare$}}}

\numberwithin{theorem}{chapter}
\numberwithin{definition}{chapter}
\numberwithin{algorithm}{chapter}
\numberwithin{figure}{chapter}
\numberwithin{table}{chapter}

% Maths
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\numberwithin{equation}{chapter}
\allowdisplaybreaks

% Shaded boxes
\usepackage{framed}
\newenvironment{remark}[1]{%
  \definecolor{shadecolor}{gray}{0.9}%
  \begin{shaded}{\color{Maroon}\noindent\textsc{#1}}\\%
}{%
  \end{shaded}%
}

% Code snippets
%\usepackage{color}

\usepackage{color}
\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\definecolor{bgcolor}{rgb}{1.0,1.0,1.0}
%\newminted{python}{bgcolor=bgcolor}

% Todo
\newcommand{\todo}[1]{\textcolor{red}{[TODO] #1}}

% PS pictures
%\usepackage{pstricks,auto-pst-pdf}

% For the images and graphics
\usepackage{subfig} % For subfigures in floats
\usepackage[section]{placeins}
\makeatletter
 \@ifpackageloaded{tex4ht}{%
\usepackage[dvips]{color,graphicx}
    \usepackage[tex4ht]{hyperref}
    }{%
      \usepackage[pdftex]{graphicx}
      \usepackage{hyperref}
          }
\makeatother
\graphicspath{ {figures/} } %Path to images


% Landscape tables
\usepackage{rotating}

% Checkmarks
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Wide tables
\usepackage{ltablex}


% -----------------------------------------------------------------------------

\begin{document}
\frenchspacing
\raggedbottom
\selectlanguage{american}
\pagenumbering{roman}
\pagestyle{plain}


% Front pages =================================================================
\include{titlepage}

% Content =====================================================================
\pagenumbering{arabic}

\cleardoublepage

%----------------------------------------------------------------------------------------
%  CHAPTER CONTENTS
%----------------------------------------------------------------------------------------

\chapter*{Introduction}

\section*{Project Overview}

This project applies unsupervised learning techniques on product spending data collected for customers of a wholesale distributor in Lisbon, Portugal to identify customer segments hidden in the data. 

\section*{Problem Statement}

A wholesale distributor recently tested a change to their delivery method for some customers, by moving from a morning delivery service five days a week to a cheaper evening delivery service three days a week. Initial testing did not discover any significant unsatisfactory results, so they implemented the cheaper option for all customers. Almost immediately, the distributor began getting complaints about the delivery service change and customers were canceling deliveries--losing the distributor more money than what was being saved.

The goal of the wholesale distributor is to find what types of customers they have to help them make better, more informed business decisions in the future.

The dataset contains 440 data points representing clients of a wholesale distributor. It includes the annual spending in monetary units on the following product categories:

\begin{itemize}% 
\item Fresh
\item Milk
\item Grocery
\item Frozen
\item Detergents\_Paper
\item Delicatessen
\end{itemize}



To find out what types of customers the wholesale distributors have we suggest the following strategy:
\begin{enumerate}% 
\item Preprocess the data by applying appropriate techniques like feature scaling and outlier detection.
\item Explore the data to determine relevant features.
\item Perform a principal component analysis on the data to understand which features seem to trend together.
\item Implement clustering to find hidden patterns in a dataset.

\end{enumerate}



\section*{Metrics}
For this problem, we don't know how many ``segments'' of customers there are. In fact, this is what we are trying to learn. As such, we can use the \emph{silhouette score} of each data point to determine how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). The \emph{mean silhouette coefficient score} can the serve as a metric that can guides us in learning the optimal number of segments in the data.



%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Analysis}

\section*{Data Exploration}

The dataset used in this project was created by UC Irvine's' Center for Machine Learning and Intelligent Systems. The data is stored in a CSV file where each row corresponds to an order.

\section*{Sample Data Points}
To gain a better understanding of the customers and how their data will transform through the analysis, we randomly selected three data points that varied significantly from each other. 

\begin{table}[!htbp]
  \begin{center}
  \caption{Samples' category spending.}
    \begin{tabular}{ |l|l|l|l|l|l|l| } 
    \hline
    Sample & Fresh & Milk & Grocery & Frozen & Detergents\_Paper & Delicatessen\\[1ex]

    \hline
    
      0 &12669 &9656  &7561  &214   &2674  &1338 \\
      1 &3067  &13240 &23127 &3941  &9959  &731  \\
      2 &32717 &16784 &13626 &60869 &1272  &5609 \\
    \hline
    \end{tabular}
    \label{tableSamplesCategorySpending}
  \end{center}
\end{table}

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.9\textwidth]{figures/percentileRanksSampleSpending}
    \label{percentileRanksSampleSpending}}
    
    \caption{\textbf{Percentile ranks of samples' category spending. }}
\end{figure}


\section*{Feature Relevance}
Let's figure out if it's possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? 

To do this, we can train a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.

We predicted the impact of the feature \texttt{Grocery} on customers' spending habits. The prediction score was 0.682 out of 1. This score which tends towards being high indicates that it this feature is not necessary for identifying customers' spending habits. The reason is as follows: 
\begin {itemize}
\item when customers spend on \texttt{Grocery} they also spend on other features, which means we can't decipher spending habits. 
\end{itemize}
What would be better is to regress on a feature that has a \textbf{low} $R^2$ score indicating low correlation.


To get a better understanding of the dataset, we can construct a scatter matrix of each of the six product features present in the data. From figure \ref{scatterplot} we can see that the data for these features is not normally distributed. Its skewed to the right and has a long tail.
\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.9\textwidth]{figures/scatterplot}
    \label{scatterplot}}
    
    \caption{\textbf{A scatter matrix for each pair of features in the data. }}
\end{figure}

The pairs \texttt{Grocery} and \texttt{Detergents\_Paper} have a very strong correlation. From the scatterplot, one can see that the data for these two features looks like it could fit a nice diagonal. To formalize my intuition, I performed a pearson's correlation on these two features and got a 0.92 score, noting that a score of 1 is the highest a correlation can be.

Another pair was \texttt{Grocery} and \texttt{Milk}. From the scatterplot, you can kind of see a trending along the diagonal. It had a correlation score of 0.73.

Another pair was \texttt{Milk} and \texttt{Detergents\_Paper}, the pair had a correlation score of 0.66. This confirms my suspicions that \texttt{Grocery} is **not** a strong predictor of overall client spending habits.




\section*{Outlier detection}
The algorithm that we will be using to cluster the data points, \texttt{KMeans} is \emph{very sensitive} to outliers. Since this algorithm uses the distance from the centroids of the clusters, i.e. the means to calculate which data points belong to what cluster, leaving values that are very far off from the rest will influence the structure, leading to undesirable clusters.

As part of the data exploration process, we were careful to analyze the data for potential outliers. We used Tukey's Method for identfying outliers to clean the data. We removed 42 data points from our dataset which corresponds to 10\% of the dataset. In this case we still have around 390 uniques data points for a problem with 6 variables. 


%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------
\chapter*{Methodology}

\section*{Feature Transformation}
We scaled the data to a more normal distribution and  removed outliers. We then applied PCA to the data to discover which dimensions about the data best maximize the variance of features involved. 

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.9\textwidth]{figures/pca}
    \label{scatterplot}}
    
    \caption{\textbf{PCA on the log transformed data. }}
\end{figure}

\section*{Implementation}
In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?
\item Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?
\item Was there any part of the coding process (e.g., writing complicated functions) that should be documented?
\end{itemize}


\section*{Refinement}
In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Has an initial solution been found and clearly reported?
\item Is the process of improvement clearly documented, such as what techniques were used?
\item Are intermediate and final solutions clearly reported as the process is improved?
\end{itemize}
%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Results}
(approximately 2 - 3 pages)


\section*{Model Evaluation and Validation}
In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?
\item Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?
\item Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?
\item Can results found from the model be trusted?
\end{itemize}

\section*{Justification}
In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Are the final results found stronger than the benchmark result reported earlier?
\item Have you thoroughly analyzed and discussed the final solution?
\item Is the final solution significant enough to have solved the problem?
\end{itemize}

%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Conclusion}
(approximately 1 - 2 pages)

\section*{Free-Form Visualization}
In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Have you visualized a relevant or important quality about the problem, dataset, input data, or results?
\item Is the visualization thoroughly analyzed and discussed?
\item If a plot is provided, are the axes, title, and datum clearly defined?
\end{itemize}


\section*{Reflection}
In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Have you thoroughly summarized the entire process you used for this project?
\item Were there any interesting aspects of the project?
\item Were there any difficult aspects of the project?
\item Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?
\end{itemize}

\section*{Improvement}
In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Are there further improvements that could be made on the algorithms or techniques you used in this project?
\item Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?
\item If you used your final solution as the new benchmark, do you think an even better solution exists?
\end{itemize}



\end{document}  